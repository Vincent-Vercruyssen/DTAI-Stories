<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DTAI</title><link>https://dtai.cs.kuleuven.be/posts/authors/thomas-winters/</link><atom:link href="https://dtai.cs.kuleuven.be/posts/authors/thomas-winters/index.xml" rel="self" type="application/rss+xml"/><description>DTAI</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 23 Mar 2020 23:52:28 +0100</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>DTAI</title><link>https://dtai.cs.kuleuven.be/posts/authors/thomas-winters/</link></image><item><title>Can computers create jokes? [DRAFT]</title><link>https://dtai.cs.kuleuven.be/posts/post/thomas-winters/computational_humor_mopjesbot/</link><pubDate>Mon, 23 Mar 2020 23:52:28 +0100</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/thomas-winters/computational_humor_mopjesbot/</guid><description>&lt;h3 id="computational-humor">Computational humor&lt;/h3>
&lt;p>Can computers be funny?
Certainly your virtual assistant (e.g. &lt;em>Siri&lt;/em> or &lt;em>Alexa&lt;/em>) is able to tell a joke if you ask for one, but these are of course pre-written, human-made jokes.
One might wonder if are computers already advanced enough to understand how to construct good jokes.
And if they could write jokes, could they do this in any language and about any topic?&lt;/p>
&lt;p>Many researchers have already looked into humor generation algorithms.
Recent popular neural networks approaches seem to indicate that
writing good jokes is
&lt;a href="https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912" target="_blank" rel="noopener">still far off&lt;/a>.
Most of the funny things computers create using neural networks,
&lt;a href="https://aiweirdness.com/books" target="_blank" rel="noopener">seem to mostly occur on accident&lt;/a>.&lt;/p>
&lt;p>Further in the past, however, more symbolic approaches have been used to generate subjectively better jokes.
Researchers created programs to for example
&lt;a href="http://joking.abdn.ac.uk/webversion/welcome.php" target="_blank" rel="noopener">generate punning riddles&lt;/a>,
&lt;a href="http://www.infoivy.com/2013/09/big-data-what-joke-generator-that-is.html" target="_blank" rel="noopener">create analogy jokes&lt;/a>
and
&lt;a href="https://www.popsci.com/technology/article/2011-04/thats-what-she-said-software-recognizes-pervy-double-entendres-automatically/" target="_blank" rel="noopener">detect double entendres&lt;/a>.
These programs usually define some rules that constitute a funny joke, and then fill in the slots randomly.
For example, the
&lt;a href="%28http://joking.abdn.ac.uk/webversion/welcome.php%29">STANDUP punning riddle generator&lt;/a> might generate a joke like:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a pretty glove and a silent cat?&lt;/p>
&lt;p>One is a cute mitten, the other is a mute kitten.&lt;/p>
&lt;/blockquote>
&lt;p>To create such a joke, the generator uses templates.
Template can be seen as sentences with holes, which are later filled in by following certain rules.
For example, for the above joke, the template would be the same as the joke, but without the words &lt;em>pretty&lt;/em>, &lt;em>glove&lt;/em>, &lt;em>silent&lt;/em>, &lt;em>cat&lt;/em>, &lt;em>cute&lt;/em>, &lt;em>mitten&lt;/em>, &lt;em>mute&lt;/em> and &lt;em>kitten&lt;/em>:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a &lt;strong>A&lt;/strong> &lt;strong>B&lt;/strong> and a &lt;strong>C&lt;/strong> &lt;strong>D&lt;/strong>?&lt;/p>
&lt;p>One is a &lt;strong>E&lt;/strong> &lt;strong>F&lt;/strong>, the other is a &lt;strong>G&lt;/strong> &lt;strong>H&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>These holes are then related to each other using rules, which then fill the holes with appropriate words.
For example: &lt;strong>E&lt;/strong> and &lt;strong>G&lt;/strong> should end with the same sound, and so should &lt;strong>F&lt;/strong> and &lt;strong>H&lt;/strong>.
Similarly, &lt;strong>E&lt;/strong> and &lt;strong>H&lt;/strong> have to start with the same sound, and so do &lt;strong>F&lt;/strong> and &lt;strong>G&lt;/strong>.
To create the question of the riddle, four pairs of synonyms are required, namely &lt;strong>A&lt;/strong> should be a synonym of &lt;strong>E&lt;/strong>, &lt;strong>B&lt;/strong> of &lt;strong>F&lt;/strong> and so on.
In a way, the generator is playing
&lt;a href="http://www.madlibs.com/" target="_blank" rel="noopener">Mad Libs&lt;/a> with itself, but enforcing slightly more logic in the relations between the words.&lt;/p>
&lt;h3 id="teaching-joke-patterns">Teaching joke patterns&lt;/h3>
&lt;p>So while it might be hard to make a computer come up with a broad range of clever jokes completely from scratch,
we can teach them how to generate specific types of jokes.
However, not all language have the rich corpora available to pull this off, like the English language does.
Another problem is that most joke generators use static data sources, and are thus unable to create jokes about topics that are not included in this data, unless they are manually updated.
Old joke generators might thus not be able to make jokes about new music artists or politicians.&lt;/p>
&lt;p>We created
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">bot&lt;/a> for generating Dutch &amp;ldquo;Kermit de Kikker&amp;rdquo; punning riddles, using limited Dutch language resources,
namely
&lt;a href="https://nl.wikipedia.org/" target="_blank" rel="noopener">Wikipedia&lt;/a>, a
&lt;a href="https://www.mijnwoordenboek.nl/synoniem.php" target="_blank" rel="noopener">thesaurus&lt;/a> (or
&lt;a href="https://nl.wiktionary.org/wiki/Hoofdpagina" target="_blank" rel="noopener">Wiktionary&lt;/a>),
&lt;a href="https://www.mijnwoordenboek.nl/rijmwoordenboek/" target="_blank" rel="noopener">rhyming dictionary&lt;/a> and
&lt;a href="https://www.ushuaia.pl/hyphen/?ln=nl" target="_blank" rel="noopener">hyphenation&lt;/a>.
All these resources tend to be available online for most languages.
The classic &lt;em>&amp;ldquo;Kermit de Kikker&amp;rdquo;&lt;/em> (&lt;em>Dutch for
&lt;a href="https://en.wikipedia.org/wiki/Kermit_the_Frog" target="_blank" rel="noopener">Kermit the Frog&lt;/a>&lt;/em>) joke is based on finding rhymes of &lt;em>Kikker&lt;/em>,
and suggesting in the riddle what word the listener should look for.
For example:&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en het plakt?&lt;/p>
&lt;p>Kermit de Sticker&lt;/p>
&lt;/blockquote>
&lt;p>Which is usually followed by a large succession of similar jokes about Kermit, e.g.&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en is pyromaan?&lt;/p>
&lt;p>Kermit de Fikker&lt;/p>
&lt;/blockquote>
&lt;p>As you probably see, this is something we can teach computers to generate for us.
But why stick with only &lt;em>Kermit de Kikker&lt;/em> when you can insert any name?
Given the name &lt;em>Kanye West&lt;/em> as input, it would perform the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>
&lt;a href="https://www.rhymezone.com/r/rhyme.cgi?Word=west&amp;amp;typeofrhyme=perfect&amp;amp;org1=syl&amp;amp;org2=l&amp;amp;org3=y" target="_blank" rel="noopener">Find a rhyme&lt;/a>
on the last word with the same number of syllables, e.g. &lt;em>rest&lt;/em> .
If it has multiple syllables, look for rhymes on any combination of consequent syllables.
Prefer more common words using a word frequency list, if this is available in the language of choice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Replace the relevant syllables of the input name with the rhyme word, e.g. Kanye Rest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use
&lt;a href="https://wikipedia.org" target="_blank" rel="noopener">Wikipedia&lt;/a> to find a nice description of the entity with the input name.
This is not that hard to find, since Wikipedia page entries usually start with &lt;em>[page_name] &lt;strong>is&lt;/strong> [explanation]&lt;/em>.
By taking the part after the &lt;em>&amp;ldquo;to be&amp;rdquo;&lt;/em> verb, and until any punctuation or start of clause, the program can distill a brief description.
For example, it would describe
&lt;a href="https://en.wikipedia.org/wiki/Kanye_West" target="_blank" rel="noopener">Kanye West&lt;/a> as &lt;em>an American rapper&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It now only has to describe the rhyme word to complete the pun riddle.
To achieve this, either a
&lt;a href="https://www.thesaurus.com/browse/rest" target="_blank" rel="noopener">thesaurus&lt;/a> (for short descriptions),
or segments from
&lt;a href="https://en.wiktionary.org/wiki/rest" target="_blank" rel="noopener">Wiktionary&lt;/a> (for longer, more interesting descriptions) could be used.
The algorithm should however make sure that the description do not contain the word to guess itself, since that would spoil the fun.
The word frequency table could also be used to choose less common (and thus more specific) descriptive words.
For example, it could describe &lt;em>rest&lt;/em> as &lt;em>&amp;ldquo;relief from work&amp;rdquo;&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Now it can fill all these words into the the template, to create the following joke:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>It&amp;rsquo;s an American rapper and is relief from work?&lt;/p>
&lt;p>Kanye Rest&lt;/p>
&lt;/blockquote>
&lt;p>These jokes tend to become more interesting once you have multiple of them, as they turn into a fun guessing game.
Luckely, given that we completely automated the generation process, the described program can easily generate many more jokes about this person.&lt;/p>
&lt;p>We build a Twitterbot,called
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">MopjesBot&lt;/a>, that generates five unique jokes using this schema on a daily basis.
It first checks the news for articles, then filters out articles about too sensitive topics, and finally picks the name that occurs most in these articles.
The steps it follows to generate these jokes are summarised in the diagram below:&lt;/p>
&lt;p>&lt;img src="mopjesbot_flow.png" alt="mopjesbot overview">&lt;/p>
&lt;p>This system is thus able to generate jokes following a specific template and schema,
but also nudges the jokes to have a higher probability of having certain characteristics (e.g. common or less common words in certain template &amp;ldquo;holes&amp;rdquo;).&lt;/p>
&lt;h3 id="learning-what-constitutes-a-joke">Learning what constitutes a joke&lt;/h3>
&lt;p>While it&amp;rsquo;s great that we can already make computers generate jokes using templates and schemas, implementing these require a large amount of human effort.
It&amp;rsquo;s not really the machine gaining insights into humor, but rather the human giving the machine explicit insights into a specific type of joke.&lt;/p>
&lt;p>So, could it learn these insights by itself?
That&amp;rsquo;s one task we want to tackle in the future, which can be subdivided in multiple parts:&lt;/p>
&lt;ul>
&lt;li>can we automatically extract relations between words&lt;/li>
&lt;li>can we then find out which are better by learning probabilities, with similar &amp;ldquo;nudging&amp;rdquo; functions?&lt;/li>
&lt;/ul>
&lt;p>The former is something we have
&lt;a href="https://www.researchgate.net/publication/325432136_Automatic_Joke_Generation_Learning_Humor_from_Examples" target="_blank" rel="noopener">explored in the past&lt;/a>, and are still actively investigating.
The latter task could be achieve using preference learning.
&lt;a href="https://en.wikipedia.org/wiki/Preference_learning" target="_blank" rel="noopener">Preference learning&lt;/a> is a task where given a set of two data points, the algorithm has to guess which one is prefered by a human.
For this, a human first has to specify for many examples which one out of two possibilities they prefer.
By just joining the attributes of the two data points and adding 0 if the left one is prefered and 1 otherwise, this become a binary classification problem.
The trained model is then a comparator that can be used to sort lists of these data points, if the type of model is chosen correctly.&lt;/p>
&lt;p>For our case, this would mean that a model would learn which out of two jokes (or parameters to construct a joke) it would prefer.
Since we know the rules constructing this joke, we could then derive information about which rules are causing better jokes in what situations.&lt;/p>
&lt;p>However, this is all still work for in the future.
For now, we will have to just keep on writing the joke generators ourselves, or just listen to pre-written jokes told by Siri.&lt;/p></description></item></channel></rss>