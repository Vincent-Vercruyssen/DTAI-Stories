<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | DTAI</title><link>https://dtai.cs.kuleuven.be/posts/post/</link><atom:link href="https://dtai.cs.kuleuven.be/posts/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 24 Mar 2020 16:58:21 +0100</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>Posts</title><link>https://dtai.cs.kuleuven.be/posts/post/</link></image><item><title>Time Series Clustering</title><link>https://dtai.cs.kuleuven.be/posts/post/wannes/time-series-clustering/</link><pubDate>Tue, 24 Mar 2020 16:58:21 +0100</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/wannes/time-series-clustering/</guid><description>&lt;p>Clustering is one of the most widely used techniques for time series because it allows to identify and summarize patterns that are of interest (e.g., frequent or anomalous patterns). Furthermore, it does not rely on costly human supervision of time-consuming labeling.
As a result, time series clustering has been studied for many different applications such as astronomy, biology, meteorology, medicine, finance, robotics, engineering, etc..&lt;/p>
&lt;h2 id="unsupervised-clustering-dtw">Unsupervised Clustering: DTW&lt;/h2>
&lt;p>Time series clustering is heavily dependent on the choice of the distance used to compare two series. Typically, one is interested in similarity between shapes represented by a series, irrespective of phase or amplitude. And while many distance measures have been proposed, it has been shown that distance measures that can deal with invariances to amplitude and phase perform particularly well.
One of the best performing similarity measures is Dynamic Time Warping (DTW).&lt;/p>
&lt;p>&lt;img src="dtw_warp.png" alt="DTW Warpring">&lt;/p>
&lt;p>The resulting clustering is robust against small changes between time series. As can be seen in the figure underneath, DTW allows a clustering that nicely groups similar series.&lt;/p>
&lt;p>&lt;img src="dtw_clustering.png" alt="DTW clustering">&lt;/p>
&lt;p>The toolbox is available at &lt;a href="https://github.com/wannesm/dtaidistance/">https://github.com/wannesm/dtaidistance/&lt;/a> .&lt;/p>
&lt;h2 id="semi-supervised-clustering-cobras">Semi-supervised Clustering: COBRAS&lt;/h2>
&lt;p>Clustering is ubiquitous in data analysis. There is a large diversity in algorithms, loss functions, similarity measures, etc. This is partly due to the fact that clustering is inherently subjective: in many cases, there is no single correct clustering, and different users may prefer different clusterings, depending on their goals and prior knowledge [17]. Depending on their preference, they should use the right algorithm, similarity measure, loss function, hyperparameter settings, etc. This requires a fair amount of knowledge and expertise on the user&amp;rsquo;s side.
Semi-supervised clustering methods deal with this subjectiveness in a differ- ent manner. They allow the user to specify constraints that express their subjective interests. These constraints can then guide the algorithm towards solutions that the user finds interesting. Many such systems obtain these constraints by asking the user to answer queries of the following type: should these two elements be in the same cluster? A so-called must-link constraint is obtained if the answer is yes, a cannot-link otherwise. In many situations, answering this type of questions is much easier for the user than selecting the right algorithm, defining the similarity measure, etc. Active semi-supervised clustering methods
aim to limit the number of queries that is required to obtain a good clustering by selecting informative pairs to query.
In the context of clustering time series, the subjectiveness of clustering is even more prominent. In some contexts, the time scale matters, in other contexts it does not. Similarly, the scale of the amplitude may (not) matter. One may want to cluster time series based on certain types of qualitative behavior (monotonic, periodic, &amp;hellip;), local patterns that occur in them, etc. Despite this variability, and although there is a plethora of work on time series clustering, semi-supervised clustering of time series has only very recently started receiving attention.&lt;/p>
&lt;p>&lt;img src="cobras_logo.png" alt="cobras">&lt;/p>
&lt;p>Clustering is ubiquitous in data analysis, including analysis of time series. But it is inherently subjective: different users may prefer different clusterings for a particular dataset. Semi-supervised clustering addresses this by allowing the user to provide examples of instances that should (not) be in the same cluster.&lt;/p>
&lt;p>The COBRAS-TS algorithm is a semi-supervised clustering method that can use indirect feedback from users. It clusters with the user in the loop. The user can provide indirect feedback where it is only required to tell the system for a few time series whether two time series represent the same behavior or not. As a result the COBRAS-TS system can identify clusters that are characterized by small local patterns.&lt;/p>
&lt;p>&lt;img src="img1.png" alt="img1">
&lt;img src="img2.png" alt="img2">&lt;/p>
&lt;p>An interface is provided to the user that actively asks for feedback about time series where the method is the most unsure about how to cluster them.&lt;/p>
&lt;p>&lt;img src="cobras_schema.png" alt="gui">
&lt;img src="cobras_example.png" alt="gui">&lt;/p>
&lt;p>The toolbox is available at &lt;a href="https://dtai.cs.kuleuven.be/software/cobras/">https://dtai.cs.kuleuven.be/software/cobras/&lt;/a> .&lt;/p></description></item><item><title>Anomaly Detection with User Feedback</title><link>https://dtai.cs.kuleuven.be/posts/post/wannes/anomaly_detection/</link><pubDate>Tue, 24 Mar 2020 10:36:34 +0200</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/wannes/anomaly_detection/</guid><description>&lt;p>Anomaly detection deals with situations where mostly &amp;lsquo;normal&amp;rsquo; behaviour is observed and one is interested in detecting deviations from this normal behaviour. Often negative labels, or anomalies, are not available because they are expensive (e.g. break a machine) or unknown (e.g. adversarial behaviour). Anomaly detection method typically learn from the distribution of data and a small number of labels to include expert knowledge.&lt;/p>
&lt;p>Nowadays, all aspects of a production process are continuously monitored and visualized in a dashboard. Equipment is monitored using a variety of sensors, natural resource usage is tracked, and interventions are recorded. In this context, a common task is to identify anomalous behavior from the time series data generated by sensors. As manually analyzing such data is laborious and expensive, automated approaches have the potential to be much more efficient as well as cost effective. While anomaly detection could be posed as a supervised learning problem, typically this is not possible as few or no labeled examples of anomalous behavior are available and it is oftentimes infeasible or undesirable to collect them. Therefore, unsupervised approaches are commonly employed which typically identify anomalies as deviations from normal (i.e., common or frequent) behavior. However, in many real-world settings several types of normal behavior exist that occur less frequently than some anomalous behaviors.&lt;/p>
&lt;p>The methods presented in this article are implemented as part of the
&lt;a href="https://github.com/Vincent-Vercruyssen/anomatools" target="_blank" rel="noopener">Anomatools toolbox&lt;/a>.&lt;/p>
&lt;h2 id="including-flexible-user-feedback">Including flexible user-feedback&lt;/h2>
&lt;h3 id="semi-supervised-learning">Semi-supervised learning&lt;/h3>
&lt;p>Sometimes there is some information available. For example, some labels might be known or the user can answer a few question.
An approach used in our research is a constrained-clustering-based approach for anomaly detection that works in both an unsupervised and semi-supervised setting. Starting from an unlabeled data set, the approach is able to gradually incorporate expert-provided feedback to improve its performance.&lt;/p>
&lt;p>&lt;img src="featured.png" alt="another image">&lt;/p>
&lt;p>References:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://people.cs.kuleuven.be/~vincent.vercruyssen/publications/2019/ECMLPKDD_conference_manuscript.pdf" target="_blank" rel="noopener">Semi-supervised Anomaly Detection with an Application to Water Analytics&lt;/a>. Vincent Vercruyssen, Wannes Meert, Gust Verbruggen, Koen Maes, Ruben BÃ¤umer, Jesse Davis. IEEE International Conference on Data Mining. Singapore, 17 November 2018.&lt;/li>
&lt;/ul>
&lt;h3 id="transfer-learning">Transfer Learning&lt;/h3>
&lt;p>References:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://people.cs.kuleuven.be/~vincent.vercruyssen/publications/2020/AAAI_conference_manuscript.pdf" target="_blank" rel="noopener">Transfer Learning for Anomaly Detection through Localized and Unsupervised Instance Selection&lt;/a>. Vincent Vercruyssen, Wannes Meert, Jesse Davis. Thirty-Fourth AAAI Conference on Artificial Intelligence. New York, 7 February 2020.&lt;/li>
&lt;/ul>
&lt;h2 id="types-of-patterns">Types of Patterns&lt;/h2>
&lt;h3 id="missing-patterns">Missing patterns&lt;/h3>
&lt;ul>
&lt;li>&amp;ldquo;Now you see it, now you don&amp;rsquo;t!&amp;rdquo; Detecting Suspicious Pattern Absences in Continuous Time Series. Vincent Vercruyssen, Wannes Meert, Jesse Davis. SIAM International Conference on Data Mining. Cincinnati, 7 May 2020.&lt;/li>
&lt;/ul></description></item><item><title>Can computers create jokes?</title><link>https://dtai.cs.kuleuven.be/posts/post/thomas-winters/computational_humor_mopjesbot/</link><pubDate>Mon, 23 Mar 2020 23:52:28 +0100</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/thomas-winters/computational_humor_mopjesbot/</guid><description>&lt;h3 id="computational-humor">Computational humor&lt;/h3>
&lt;p>Can computers be funny?
Certainly your virtual assistant (e.g. &lt;em>Siri&lt;/em> or &lt;em>Alexa&lt;/em>) is able to tell a joke if you ask for one, but these are of course pre-written, human-made jokes.
One might wonder if computers are already advanced enough to understand how to construct good jokes.
And if they can write jokes, can they do this in any language and about any topic?
And could they tailor their sense of humor to users?&lt;/p>
&lt;p>Many researchers have already looked into humor generation algorithms.
Recent popular neural networks approaches seem to indicate that
writing good jokes is
&lt;a href="https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912" target="_blank" rel="noopener">still far off&lt;/a>.
Most of the funny things computers create using neural networks,
&lt;a href="https://aiweirdness.com/books" target="_blank" rel="noopener">seem to mostly occur on accident&lt;/a>.&lt;/p>
&lt;p>Further in the past, however, more symbolic approaches have been used to generate subjectively better jokes.
Researchers created programs to, for example,
&lt;a href="http://joking.abdn.ac.uk/webversion/welcome.php" target="_blank" rel="noopener">generate punning riddles&lt;/a>,
&lt;a href="http://www.infoivy.com/2013/09/big-data-what-joke-generator-that-is.html" target="_blank" rel="noopener">create analogy jokes&lt;/a>
and
&lt;a href="https://www.popsci.com/technology/article/2011-04/thats-what-she-said-software-recognizes-pervy-double-entendres-automatically/" target="_blank" rel="noopener">detect double entendres&lt;/a>.
These programs usually define some rules that constitute a funny joke, and then fill in the slots randomly.
For example, the
&lt;a href="%28http://joking.abdn.ac.uk/webversion/welcome.php%29">STANDUP punning riddle generator&lt;/a> might generate a joke like:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a pretty glove and a silent cat?&lt;/p>
&lt;p>One is a cute mitten, the other is a mute kitten.&lt;/p>
&lt;/blockquote>
&lt;p>To create such a joke, the generator uses templates.
Template can be seen as sentences with holes, which are later filled in by following certain rules.
For example, for the above joke, the template would be the same as the joke, but without the words &lt;em>pretty&lt;/em>, &lt;em>glove&lt;/em>, &lt;em>silent&lt;/em>, &lt;em>cat&lt;/em>, &lt;em>cute&lt;/em>, &lt;em>mitten&lt;/em>, &lt;em>mute&lt;/em> and &lt;em>kitten&lt;/em>:&lt;/p>
&lt;blockquote>
&lt;p>What is the difference between a &lt;strong>A&lt;/strong> &lt;strong>B&lt;/strong> and a &lt;strong>C&lt;/strong> &lt;strong>D&lt;/strong>?&lt;/p>
&lt;p>One is a &lt;strong>E&lt;/strong> &lt;strong>F&lt;/strong>, the other is a &lt;strong>G&lt;/strong> &lt;strong>H&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>These holes are then related to each other using rules, which then fill the holes with appropriate words.
For the above template, there are constraints enforcing that &lt;strong>E&lt;/strong> and &lt;strong>G&lt;/strong> end with the same sound, and so should &lt;strong>F&lt;/strong> and &lt;strong>H&lt;/strong>.
Similarly, &lt;strong>E&lt;/strong> and &lt;strong>H&lt;/strong> start with the same sound, and so do &lt;strong>F&lt;/strong> and &lt;strong>G&lt;/strong>.
To create the question of the riddle, four pairs of synonyms are required, namely &lt;strong>A&lt;/strong> should be a synonym of &lt;strong>E&lt;/strong>, &lt;strong>B&lt;/strong> of &lt;strong>F&lt;/strong> and so on.
In a way, the generator is playing
&lt;a href="http://www.madlibs.com/" target="_blank" rel="noopener">Mad Libs&lt;/a> with itself, but enforcing slightly more logic in the relations between the words.&lt;/p>
&lt;p>&lt;img src="mopjesbot_drawing.jpg" alt="mopjesbot">&lt;/p>
&lt;h3 id="teaching-joke-patterns">Teaching joke patterns&lt;/h3>
&lt;p>So while it might be hard to make a computer come up with a broad range of clever jokes completely from scratch,
we can teach them how to generate specific types of jokes.
However, while many English language resources exist, not all languages possess such plentiful language resources for enforcing and checking linguistic constraints.
Another problem is that most joke generators use static data sources (e.g. an internal dictionary), and are thus unable to create jokes about topics that are not included in this data, unless they are manually updated.
Old joke generators might thus not be able to make jokes about new music artists or politicians.&lt;/p>
&lt;p>We created
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">bot&lt;/a> for generating Dutch &lt;em>&amp;ldquo;Kermit de Kikker&amp;rdquo;&lt;/em> punning riddles, using limited Dutch language resources,
namely
&lt;a href="https://nl.wikipedia.org/" target="_blank" rel="noopener">Wikipedia&lt;/a>, a
&lt;a href="https://www.mijnwoordenboek.nl/synoniem.php" target="_blank" rel="noopener">thesaurus&lt;/a> (or
&lt;a href="https://nl.wiktionary.org/wiki/Hoofdpagina" target="_blank" rel="noopener">Wiktionary&lt;/a>),
&lt;a href="https://www.mijnwoordenboek.nl/rijmwoordenboek/" target="_blank" rel="noopener">rhyming dictionary&lt;/a> and
&lt;a href="https://www.ushuaia.pl/hyphen/?ln=nl" target="_blank" rel="noopener">hyphenation&lt;/a>.
All these resources tend to be available online for most of the popular languages.
The classic &lt;em>&amp;ldquo;Kermit de Kikker&amp;rdquo;&lt;/em> (&lt;em>Dutch for
&lt;a href="https://en.wikipedia.org/wiki/Kermit_the_Frog" target="_blank" rel="noopener">Kermit the Frog&lt;/a>&lt;/em>) joke is based on finding rhymes of &lt;em>Kikker&lt;/em> based on what the riddle suggest.
For example:&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en het plakt?&lt;/p>
&lt;p>Kermit de Sticker&lt;/p>
&lt;/blockquote>
&lt;p>In English, this joke says: &lt;em>&amp;ldquo;It&amp;rsquo;s green and adhesive? Kermit the Sticker&amp;rdquo;&lt;/em>.
This is a joke because &lt;em>&amp;ldquo;sticker&amp;rdquo;&lt;/em> is a rhyme of &lt;em>&amp;ldquo;kikker&amp;rdquo;&lt;/em>, Dutch for &lt;em>&amp;ldquo;frog&amp;rdquo;&lt;/em>.
Usually, this joke is followed by a large succession of similar jokes about Kermit, e.g.&lt;/p>
&lt;blockquote>
&lt;p>Het is groen en is pyromaan?&lt;/p>
&lt;p>Kermit de Fikker&lt;/p>
&lt;/blockquote>
&lt;p>As you might have realised, this is something we can teach computers to generate for us.
But why stick with only making jokes about &lt;em>Kermit de Kikker&lt;/em> when you can insert any name?
Given the name &lt;em>Kanye West&lt;/em> as input, it would perform the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>
&lt;a href="https://www.rhymezone.com/r/rhyme.cgi?Word=west&amp;amp;typeofrhyme=perfect&amp;amp;org1=syl&amp;amp;org2=l&amp;amp;org3=y" target="_blank" rel="noopener">Find a rhyme&lt;/a>
on the last word with the same number of syllables, e.g. &lt;em>rest&lt;/em> .
If the last word of the input has multiple syllables, look for rhymes on any combination of consequent syllables.
Prefer more common words using a word frequency list, if this is available in the language of choice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Replace the relevant syllables of the input name with the rhyme word, e.g. Kanye Rest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use
&lt;a href="https://wikipedia.org" target="_blank" rel="noopener">Wikipedia&lt;/a> to find a nice description of the entity with the input name.
This is not that hard to extract from the Wikipedia page, since the introduction usually start with &lt;em>[entity_name] &lt;strong>is/was/are&lt;/strong> [explanation]&lt;/em>.
By taking the part after the &lt;em>&amp;ldquo;to be&amp;rdquo;&lt;/em> verb, and until any punctuation or start of clause, the program can distill a brief description.
For example, it would describe
&lt;a href="https://en.wikipedia.org/wiki/Kanye_West" target="_blank" rel="noopener">Kanye West&lt;/a> as &lt;em>an American rapper&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It now only has to describe the rhyme word to complete the pun riddle.
To achieve this, either a
&lt;a href="https://www.thesaurus.com/browse/rest" target="_blank" rel="noopener">thesaurus&lt;/a> (for short descriptions),
or segments from
&lt;a href="https://en.wiktionary.org/wiki/rest" target="_blank" rel="noopener">Wiktionary&lt;/a> (for longer, more interesting descriptions) could be used.
The algorithm should however make sure that the description do not contain the word to guess itself, since that would spoil the fun.
The word frequency table could also be used to choose less common (and thus more specific) descriptive words.
For example, it could describe &lt;em>rest&lt;/em> as &lt;em>&amp;ldquo;relief from work&amp;rdquo;&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Now it can fill all these words into the the template, to create the following joke:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>It&amp;rsquo;s an American rapper and is relief from work?&lt;/p>
&lt;p>Kanye Rest&lt;/p>
&lt;/blockquote>
&lt;p>These jokes tend to become more interesting once you have multiple of them, as they turn into a fun guessing game.
Luckely, given that we completely automated the generation process, the described program can easily generate many more jokes about this person.&lt;/p>
&lt;p>We build a Twitterbot, called
&lt;a href="https://twitter.com/MopjesBot" target="_blank" rel="noopener">MopjesBot&lt;/a>, that generates five unique jokes using this schema on a daily basis.
It first checks the news for articles, then filters out articles about too sensitive topics, and finally picks the name that occurs most in these articles.
The complete overview of all steps it follows to generate these jokes are summarised in the diagram below:&lt;/p>
&lt;p>&lt;img src="mopjesbot_flow.png" alt="mopjesbot overview">&lt;/p>
&lt;p>This system is thus able to generate jokes following a specific template and schema,
but also nudges the jokes to have a higher probability of having certain characteristics (e.g. common or less common words in certain template &amp;ldquo;holes&amp;rdquo;).&lt;/p>
&lt;h3 id="learning-what-constitutes-a-joke">Learning what constitutes a joke&lt;/h3>
&lt;p>While it&amp;rsquo;s wonderful that we can already make computers generate jokes using templates and schemas, implementing such joke generators requires a large amount of human effort.
The computer is also not really gaining insights into humor itself, but rather the human giving the machine explicit insights into a specific type of joke.&lt;/p>
&lt;p>So, could it learn these insights by itself?
This is one task we want to tackle in the future, which can be subdivided in multiple parts:&lt;/p>
&lt;ul>
&lt;li>can we automatically extract meaningful relations between words of a good joke?&lt;/li>
&lt;li>can we find out which jokes are better than others by learning probabilities, and use these to &amp;ldquo;nudge&amp;rdquo; the generators into generating better jokes?&lt;/li>
&lt;/ul>
&lt;p>The former is something we have
&lt;a href="https://www.researchgate.net/publication/325432136_Automatic_Joke_Generation_Learning_Humor_from_Examples" target="_blank" rel="noopener">explored in the past&lt;/a>, and are still actively investigating.
The latter task could be achieve using preference learning.
&lt;a href="https://en.wikipedia.org/wiki/Preference_learning" target="_blank" rel="noopener">Preference learning&lt;/a> is a task where given a set of two data points, the algorithm has to predict which one is preferred by a human.
This could then be used to find optimal parameters to contruct a joke that a particular user or group of users might like.&lt;/p>
&lt;p>The future of automatic joke generation is thus exciting, and still full of opportunities.
We can already create joke generation algorithms by hand and getting close to learn them automatically.
In the future, we might not need to listen to pre-written jokes told by Siri any more, and instead could enjoy personalised, generated humor.
Or, as our algorithm might say:&lt;/p>
&lt;blockquote>
&lt;p>It&amp;rsquo;s a branch of artificial intelligence which uses computers in humor research and is a claim of questionable accuracy?&lt;/p>
&lt;p>Computational Rumor&lt;/p>
&lt;/blockquote></description></item><item><title>DeepProbLog</title><link>https://dtai.cs.kuleuven.be/posts/post/robin-manhaeve/deepproblog/</link><pubDate>Sat, 03 Aug 2019 10:36:34 +0200</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/robin-manhaeve/deepproblog/</guid><description>&lt;h1 id="deepproblog">DeepProbLog&lt;/h1>
&lt;p>DeepProbLog is a neuro-symbolic framework that integrates the probabilistic logic programming language ProbLog with neural networks.&lt;/p>
&lt;p>The full paper can be found
&lt;a href="https://arxiv.org/abs/1907.08194" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>The main strengths of DeepProbLog are:&lt;/p>
&lt;ul>
&lt;li>It combines probabilistic reasoning, logical reasoning and the power of neural networks.&lt;/li>
&lt;li>It can train neural networks and learn probabilistic paramters from examples.&lt;/li>
&lt;li>We retain both logic and neural networks as edge cases.&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-deepproblog">What is DeepProbLog?&lt;/h2>
&lt;p>DeepProbLog is an extension of ProbLog that integrates neural networks through the concept of the neural predicate. It allows us to combine high-level logical reasoning with the sub-symbolic power of neural networks.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>nn(mnist_classifier,[X],Y,[0..9]) :: digit(X,Y).
&lt;/code>&lt;/pre>
&lt;p>The neural predicate defined above declares that there&amp;rsquo;s a neural network that will take in an input X, and has at its output a probability distribution. This relation can be used in the remainder of the program using the digit relation.&lt;/p>
&lt;p>TODO: ADD figure for digit predicate distribution&lt;/p>
&lt;p>We could for example define the addition over two MNIST digits:&lt;/p>
&lt;pre>&lt;code>nn(mnist_classifier,[X],Y,[0..9]) :: digit(X,Y).
addition(X,Y,Z) :- digit(X,N1), digit(Y,N2), Z is N1+N2.
&lt;/code>&lt;/pre>
&lt;p>Where the addition relation now defines the probability distribution over the sum of the individual digits.&lt;/p>
&lt;p>TODO: add figure for addition predicate distribution&lt;/p>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;h4 id="mnist-addition">MNIST addition&lt;/h4>
&lt;p>We compared the MNIST addition example describe above with a convolutional neural network baseline.
The result shows that the inclusion of the logic allows the model to train quicker and achieve a higher accuracy. It&amp;rsquo;s also important to note that the neural network trained inside the DeepProbLog model can recognize single digits, whereas the convolutional baselines can only classify sums. The separation between the logic and neural aspects results in a more flexible model.
&lt;img src="mnist.png" alt="MNISTS result">&lt;/p>
&lt;h4 id="sketching">Sketching&lt;/h4>
&lt;p>We reimplemented the experiments from the Differentiable Forth paper. These use a sketching approach to learn the missing behaviour from partial programs using small neural modules.&lt;/p>
&lt;p>From the result we can see that we perform similar to the original (neural) model. For learning to sort lists, Differentiable Forth starts to struggle starting from length 4. This is due to the long program trace. DeepProbLog does not have this problem thanks to the fact that it can perform almost all of the program in the logic.
&lt;img src="d4.png" alt="Differentiable Forth result">&lt;/p></description></item></channel></rss>